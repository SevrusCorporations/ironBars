{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documentation ironBars Convert a Google Sheets, GitHub, or direct CSV link to a CSV-ready URL and optionally load as DataFrame(s). Parameters: Name Type Description Default url str | list URL(s) of CSV file(s). required as_df bool If True, return pandas DataFrame(s) instead of URL(s). False max_workers int | None Max threads for parallel downloading (only for list of URLs). None Returns: Type Description str | list | pd.DataFrame | list[pd.DataFrame] Source code in ironBars/ironSheets.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def gsheet_load ( url , as_df = False , max_workers = None ): \"\"\" Convert a Google Sheets, GitHub, or direct CSV link to a CSV-ready URL and optionally load as DataFrame(s). Parameters: url (str | list): URL(s) of CSV file(s). as_df (bool): If True, return pandas DataFrame(s) instead of URL(s). max_workers (int | None): Max threads for parallel downloading (only for list of URLs). Returns: str | list | pd.DataFrame | list[pd.DataFrame] \"\"\" def convert_url ( single_url ): parsed = urlparse ( single_url ) netloc = parsed . netloc . lower () path = parsed . path . lower () # Google Sheets if \"docs.google.com\" in netloc and \"/spreadsheets/\" in path : if \"/edit\" in single_url : return single_url . split ( \"/edit\" )[ 0 ] + \"/gviz/tq?tqx=out:csv\" else : return single_url # GitHub elif \"github.com\" in netloc : if \"/blob/\" in single_url : return single_url . replace ( \"github.com\" , \"raw.githubusercontent.com\" ) . replace ( \"/blob/\" , \"/\" ) return single_url # Already raw URL # Direct CSV link else : return single_url def fetch_csv ( single_url ): csv_url = convert_url ( single_url ) try : return pd . read_csv ( csv_url ) except Exception as e : raise RuntimeError ( f \"Failed to read CSV from { single_url } : { e } \" ) # Handle single URL vs list if isinstance ( url , list ): if as_df : with ThreadPoolExecutor ( max_workers = max_workers ) as executor : return list ( executor . map ( fetch_csv , url )) else : return [ convert_url ( u ) for u in url ] else : converted = convert_url ( url ) return pd . read_csv ( converted ) if as_df else converted Save one or more DataFrames as CSV files. Parameters: Name Type Description Default data_frames DataFrame | list [ DataFrame ] Single dataframe or list of dataframes to save. required auto_name bool If True, generates filenames automatically using name_series + index (only for multiple frames). True name_series str | list Base name (str) or list of names (required if auto_name=False and multiple frames). 'Sheet' save_dir str Directory to save CSV files. Defaults to current directory. '.' filename str | None Filename for single dataframe. Required if saving a single DataFrame. None Source code in ironBars/ironSheets.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def gsheet_save ( data_frames , auto_name = True , name_series = \"Sheet\" , save_dir = \".\" , filename = None ): \"\"\" Save one or more DataFrames as CSV files. Parameters: data_frames (pd.DataFrame | list[pd.DataFrame]): Single dataframe or list of dataframes to save. auto_name (bool): If True, generates filenames automatically using name_series + index (only for multiple frames). name_series (str | list): Base name (str) or list of names (required if auto_name=False and multiple frames). save_dir (str): Directory to save CSV files. Defaults to current directory. filename (str | None): Filename for single dataframe. Required if saving a single DataFrame. \"\"\" # Normalize input to list single_input = False if isinstance ( data_frames , pd . DataFrame ): data_frames = [ data_frames ] single_input = True assert data_frames , \"DATA FRAMES must not be empty!\" # Ensure absolute save directory path save_dir = os . path . abspath ( save_dir ) os . makedirs ( save_dir , exist_ok = True ) # Handle single dataframe case if single_input : assert filename , \"Must provide 'filename' when saving a single DataFrame\" if not filename . lower () . endswith ( \".csv\" ): filename += \".csv\" fpath = os . path . join ( save_dir , filename ) data_frames [ 0 ] . to_csv ( fpath , index = False ) return # Nothing to return # Handle multiple dataframes if auto_name : if isinstance ( name_series , list ): raise RuntimeError ( \"Auto Name -> True but name_series is a list! Provide a string base name instead.\" ) else : if not isinstance ( name_series , list ): raise RuntimeError ( \"Auto Name -> False. Provide a list of filenames as name_series.\" ) if len ( name_series ) < len ( data_frames ): raise ValueError ( \"Number of filenames provided is less than number of data frames.\" ) for idx , frame in enumerate ( data_frames ): if auto_name : fname = f \" { name_series }{ idx } .csv\" else : fname = f \" { name_series [ idx ] } .csv\" fpath = os . path . join ( save_dir , fname ) frame . to_csv ( fpath , index = False ) Fill NaN values in a DataFrame column or all numeric columns using either 'perlin' noise or 'linear' regression. df : pandas.DataFrame Input DataFrame containing numeric columns. column_name : str, optional Column to fill. If None, all numeric columns will be filled. method : str, default \"perlin\" Filling method: \"perlin\" for Perlin noise, \"linear\" for linear regression. seed : int, default 0 Seed or starting index for Perlin noise generation. scale_factor : float, default 0.1 Step size for Perlin noise generation. lim_min : float, default 0 Minimum limit for filled values. lim_max : float, default 100 Maximum limit for filled values. pandas.DataFrame A copy of the original DataFrame with NaNs filled, respecting user-defined limits. Raises: - TypeError: If the specified column is not numeric. - ValueError: If the method is not \"perlin\" or \"linear\". Source code in ironBars/ironSheets.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def fill_nans ( df , column_name = None , method = \"perlin\" , seed = 0 , scale_factor = 0.1 , lim_min = 0 , lim_max = 100 ): \"\"\" Fill NaN values in a DataFrame column or all numeric columns using either 'perlin' noise or 'linear' regression. Parameters: - df : pandas.DataFrame Input DataFrame containing numeric columns. - column_name : str, optional Column to fill. If None, all numeric columns will be filled. - method : str, default \"perlin\" Filling method: \"perlin\" for Perlin noise, \"linear\" for linear regression. - seed : int, default 0 Seed or starting index for Perlin noise generation. - scale_factor : float, default 0.1 Step size for Perlin noise generation. - lim_min : float, default 0 Minimum limit for filled values. - lim_max : float, default 100 Maximum limit for filled values. Returns: - pandas.DataFrame A copy of the original DataFrame with NaNs filled, respecting user-defined limits. Raises: - TypeError: If the specified column is not numeric. - ValueError: If the method is not \"perlin\" or \"linear\". \"\"\" df_copy = df . copy () if column_name is not None : if not np . issubdtype ( df_copy [ column_name ] . dtype , np . number ): raise TypeError ( f \"Column ' { column_name } ' must be numeric.\" ) cols_to_fill = [ column_name ] else : cols_to_fill = df_copy . select_dtypes ( include = [ np . number ]) . columns . tolist () for col in cols_to_fill : arr = df_copy [ col ] . to_numpy ( dtype = float ) nan_mask = np . isnan ( arr ) if not nan_mask . any (): continue if method == \"perlin\" : mean_val = np . nanmean ( arr ) std_val = np . nanstd ( arr ) indices = np . arange ( len ( arr )) noise_vals = np . array ([ pnoise1 (( i + seed ) * scale_factor ) for i in indices ]) noise_scaled = noise_vals * 2 * std_val + mean_val noise_scaled = np . clip ( noise_scaled , lim_min , lim_max ) arr [ nan_mask ] = noise_scaled [ nan_mask ] elif method == \"linear\" : indices_all = np . arange ( len ( arr )) . reshape ( - 1 , 1 ) X_train = indices_all [ ~ nan_mask ] y_train = arr [ ~ nan_mask ] X_pred = indices_all [ nan_mask ] model = LinearRegression () model . fit ( X_train , y_train ) arr [ nan_mask ] = model . predict ( X_pred ) arr = np . clip ( arr , lim_min , lim_max ) else : raise ValueError ( \"Method must be 'perlin' or 'linear'\" ) df_copy [ col ] = arr return df_copy byteBars A disk-backed, compressed, lazy-loading DataFrame-like storage system. Supports row-wise and column-wise access, vectorized operations, appending new data, and SHA-256 integrity verification. Source code in ironBars/byteBars.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 class byteBars : \"\"\" A disk-backed, compressed, lazy-loading DataFrame-like storage system. Supports row-wise and column-wise access, vectorized operations, appending new data, and SHA-256 integrity verification. \"\"\" HASH_SIZE = 64 # SHA-256 hex string length def __init__ ( self , data_file , index_file , block_size = 100 , load_existing = True ): \"\"\" Initialize the LazyCompressedDataFrame. Parameters: ----------- data_file : str Path to the binary data file. index_file : str Path to the index file storing block metadata. block_size : int Number of rows per block (for compression). load_existing : bool Whether to load existing files if present. \"\"\" self . data_file = data_file self . index_file = index_file self . block_size = block_size self . index = [] self . pending_block = [] self . _last_block_idx = None self . _last_block_data = None self . columns = None # Initialize files if they do not exist if not os . path . exists ( self . data_file ): with open ( self . data_file , \"wb\" ) as f : f . write ( b \"0\" * self . HASH_SIZE ) if not os . path . exists ( self . index_file ): with open ( self . index_file , \"wb\" ) as f : f . write ( b \"0\" * self . HASH_SIZE ) if load_existing : self . load_existing_files () # ------------------------------ # Hash utilities # ------------------------------ def _compute_hash ( self , file_path ): \"\"\" Compute SHA-256 hash of the file, skipping the header. Parameters: ----------- file_path : str Path to the file. Returns: -------- str : SHA-256 hex digest. \"\"\" hasher = hashlib . sha256 () with open ( file_path , \"rb\" ) as f : f . seek ( self . HASH_SIZE ) for chunk in iter ( lambda : f . read ( 8192 ), b \"\" ): hasher . update ( chunk ) return hasher . hexdigest () def _write_header ( self , file_path , hash_str ): \"\"\" Write a hash string to the header of a file. Parameters: ----------- file_path : str Path to the file. hash_str : str SHA-256 hash string to write. \"\"\" with open ( file_path , \"r+b\" ) as f : f . seek ( 0 ) f . write ( hash_str . encode ( \"utf-8\" )) def _read_header ( self , file_path ): \"\"\" Read the SHA-256 hash from the header of a file. Parameters: ----------- file_path : str Path to the file. Returns: -------- str : Hash string stored in header. \"\"\" with open ( file_path , \"rb\" ) as f : f . seek ( 0 ) return f . read ( self . HASH_SIZE ) . decode ( \"utf-8\" ) def validate_files ( self ): \"\"\" Validate that data and index files match each other using hashes. Raises ValueError if inconsistency is detected. \"\"\" data_hash_in_header = self . _read_header ( self . data_file ) index_hash_in_header = self . _read_header ( self . index_file ) current_data_hash = self . _compute_hash ( self . data_file ) current_index_hash = self . _compute_hash ( self . index_file ) if data_hash_in_header != current_index_hash : raise ValueError ( \"Data file header does not match current index file!\" ) if index_hash_in_header != current_data_hash : raise ValueError ( \"Index file header does not match current data file!\" ) # ------------------------------ # Load existing files # ------------------------------ def load_existing_files ( self ): \"\"\" Load index and column information from existing files if present. Validates files first. \"\"\" if os . path . getsize ( self . data_file ) > self . HASH_SIZE and os . path . getsize ( self . index_file ) > self . HASH_SIZE : self . validate_files () with open ( self . index_file , \"rb\" ) as f : f . seek ( self . HASH_SIZE ) self . index = pickle . load ( f ) if self . index : self . columns = self . _load_block ( 0 )[ 0 ] . keys () # ------------------------------ # Core methods # ------------------------------ def add_row ( self , row ): \"\"\" Add a single row to the pending block. Parameters: ----------- row : dict or pd.Series Row data to append. Columns are inferred from the first row. \"\"\" if isinstance ( row , pd . Series ): row = row . to_dict () elif not isinstance ( row , dict ): raise TypeError ( \"Row must be dict or pd.Series\" ) if self . columns is None : self . columns = list ( row . keys ()) self . pending_block . append ( row ) if len ( self . pending_block ) >= self . block_size : self . _flush_block () def add_dataframe ( self , df ): \"\"\" Add a Pandas DataFrame to the store. Parameters: ----------- df : pd.DataFrame DataFrame to append. \"\"\" if self . columns is None : self . columns = df . columns . tolist () for _ , row in df . iterrows (): self . add_row ( row ) def _flush_block ( self ): \"\"\" Compress and write the pending block to disk, update index and headers. \"\"\" if not self . pending_block : return serialized = pickle . dumps ( self . pending_block ) compressed = zlib . compress ( serialized , level = 9 ) with open ( self . data_file , \"ab\" ) as f : offset = f . tell () f . write ( compressed ) length = len ( compressed ) self . index . append (( offset , length , len ( self . pending_block ))) self . pending_block = [] self . _save_index () self . _update_headers () self . _last_block_idx = None self . _last_block_data = None def _save_index ( self ): \"\"\"Serialize and save the index to the index file.\"\"\" with open ( self . index_file , \"r+b\" ) as f : f . seek ( self . HASH_SIZE ) pickle . dump ( self . index , f ) def _update_headers ( self ): \"\"\"Update the SHA-256 headers for integrity check.\"\"\" data_hash = self . _compute_hash ( self . index_file ) index_hash = self . _compute_hash ( self . data_file ) self . _write_header ( self . data_file , data_hash ) self . _write_header ( self . index_file , index_hash ) def _load_block ( self , block_idx ): \"\"\" Load a block from disk (decompress) or return cached. Parameters: ----------- block_idx : int Index of block to load. Returns: -------- list : List of row dictionaries in the block. \"\"\" if self . _last_block_idx == block_idx : return self . _last_block_data block_offset , block_length , _ = self . index [ block_idx ] with open ( self . data_file , \"rb\" ) as f : f . seek ( block_offset ) compressed = f . read ( block_length ) block_data = pickle . loads ( zlib . decompress ( compressed )) self . _last_block_idx = block_idx self . _last_block_data = block_data return block_data # ------------------------------ # Retrieval # ------------------------------ def retrieve_row ( self , idx ): \"\"\" Retrieve a single row by global index. Parameters: ----------- idx : int Row index. Returns: -------- dict : Row data as a dictionary. \"\"\" running_total = 0 for block_idx , ( _ , _ , num_entries ) in enumerate ( self . index ): if running_total + num_entries > idx : return self . _load_block ( block_idx )[ idx - running_total ] running_total += num_entries pending_idx = idx - running_total if 0 <= pending_idx < len ( self . pending_block ): return self . pending_block [ pending_idx ] raise IndexError ( \"Index out of range\" ) def retrieve_block ( self , block_idx , as_dataframe = True ): \"\"\" Retrieve an entire block. Parameters: ----------- block_idx : int Block index. as_dataframe : bool Return as pd.DataFrame if True, else list of dicts. Returns: -------- pd.DataFrame or list of dict \"\"\" block = self . _load_block ( block_idx ) if as_dataframe : return pd . DataFrame ( block , columns = self . columns ) return block def retrieve_rows ( self , start , end ): \"\"\" Retrieve rows from start to end. Parameters: ----------- start : int end : int Returns: -------- pd.DataFrame : Rows in range [start, end) \"\"\" rows = [ self . retrieve_row ( i ) for i in range ( start , end )] return pd . DataFrame ( rows , columns = self . columns ) # ------------------------------ # Lazy DataFrame-like interface # ------------------------------ @property def df ( self ): \"\"\"Provides a lazy Pandas-like view of the dataset.\"\"\" return self . LazyView ( self ) class LazyColumn : \"\"\"Represents a lazy-access column for vectorized operations.\"\"\" def __init__ ( self , store , column ): self . store = store self . column = column def to_numpy ( self , row_slice = None ): \"\"\" Return column data as NumPy array for specified row slice. Parameters: ----------- row_slice : int, slice, or list/array, optional Returns: -------- np.ndarray \"\"\" if row_slice is None : row_slice = slice ( 0 , len ( self . store )) if isinstance ( row_slice , int ): return np . array ([ self . store . retrieve_row ( row_slice )[ self . column ]]) elif isinstance ( row_slice , slice ): start , stop , step = row_slice . indices ( len ( self . store )) data = [ self . store . retrieve_row ( i )[ self . column ] for i in range ( start , stop , step )] return np . array ( data ) elif isinstance ( row_slice , list ) or isinstance ( row_slice , np . ndarray ): data = [ self . store . retrieve_row ( i )[ self . column ] for i in row_slice ] return np . array ( data ) else : raise TypeError ( \"Unsupported index type\" ) class LazyView : \"\"\"Provides lazy DataFrame-like access.\"\"\" def __init__ ( self , store ): self . store = store def __getitem__ ( self , col ): \"\"\"Return LazyColumn for a column.\"\"\" if col not in self . store . columns : raise KeyError ( f \"Column { col } does not exist\" ) return byteBars . LazyColumn ( self . store , col ) def iloc ( self , row_slice ): \"\"\"Return rows according to integer-location-based slicing.\"\"\" if isinstance ( row_slice , int ): return pd . DataFrame ([ self . store . retrieve_row ( row_slice )], columns = self . store . columns ) elif isinstance ( row_slice , slice ): start , stop , step = row_slice . indices ( len ( self . store )) return self . store . retrieve_rows ( start , stop ) . iloc [:: step ] elif isinstance ( row_slice , list ) or isinstance ( row_slice , np . ndarray ): rows = [ self . store . retrieve_row ( i ) for i in row_slice ] return pd . DataFrame ( rows , columns = self . store . columns ) else : raise TypeError ( \"Unsupported index type for iloc\" ) def head ( self , n = 5 ): \"\"\"Return first n rows.\"\"\" return self . iloc ( slice ( 0 , n )) def tail ( self , n = 5 ): \"\"\"Return last n rows.\"\"\" return self . iloc ( slice ( len ( self . store ) - n , len ( self . store ))) def to_numpy ( self , row_slice = None ): \"\"\"Return entire view as NumPy array.\"\"\" df = self . iloc ( row_slice if row_slice else slice ( 0 , len ( self . store ))) return df . to_numpy () # ------------------------------ # Utilities # ------------------------------ def flush ( self ): \"\"\"Flush pending rows to disk.\"\"\" self . _flush_block () def __len__ ( self ): \"\"\"Return total number of rows (including pending rows).\"\"\" total = sum ( num for _ , _ , num in self . index ) total += len ( self . pending_block ) return total df property Provides a lazy Pandas-like view of the dataset. LazyColumn Represents a lazy-access column for vectorized operations. Source code in ironBars/byteBars.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 class LazyColumn : \"\"\"Represents a lazy-access column for vectorized operations.\"\"\" def __init__ ( self , store , column ): self . store = store self . column = column def to_numpy ( self , row_slice = None ): \"\"\" Return column data as NumPy array for specified row slice. Parameters: ----------- row_slice : int, slice, or list/array, optional Returns: -------- np.ndarray \"\"\" if row_slice is None : row_slice = slice ( 0 , len ( self . store )) if isinstance ( row_slice , int ): return np . array ([ self . store . retrieve_row ( row_slice )[ self . column ]]) elif isinstance ( row_slice , slice ): start , stop , step = row_slice . indices ( len ( self . store )) data = [ self . store . retrieve_row ( i )[ self . column ] for i in range ( start , stop , step )] return np . array ( data ) elif isinstance ( row_slice , list ) or isinstance ( row_slice , np . ndarray ): data = [ self . store . retrieve_row ( i )[ self . column ] for i in row_slice ] return np . array ( data ) else : raise TypeError ( \"Unsupported index type\" ) to_numpy ( row_slice = None ) Return column data as NumPy array for specified row slice. Parameters: row_slice : int, slice, or list/array, optional Returns: np.ndarray Source code in ironBars/byteBars.py 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 def to_numpy ( self , row_slice = None ): \"\"\" Return column data as NumPy array for specified row slice. Parameters: ----------- row_slice : int, slice, or list/array, optional Returns: -------- np.ndarray \"\"\" if row_slice is None : row_slice = slice ( 0 , len ( self . store )) if isinstance ( row_slice , int ): return np . array ([ self . store . retrieve_row ( row_slice )[ self . column ]]) elif isinstance ( row_slice , slice ): start , stop , step = row_slice . indices ( len ( self . store )) data = [ self . store . retrieve_row ( i )[ self . column ] for i in range ( start , stop , step )] return np . array ( data ) elif isinstance ( row_slice , list ) or isinstance ( row_slice , np . ndarray ): data = [ self . store . retrieve_row ( i )[ self . column ] for i in row_slice ] return np . array ( data ) else : raise TypeError ( \"Unsupported index type\" ) LazyView Provides lazy DataFrame-like access. Source code in ironBars/byteBars.py 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 class LazyView : \"\"\"Provides lazy DataFrame-like access.\"\"\" def __init__ ( self , store ): self . store = store def __getitem__ ( self , col ): \"\"\"Return LazyColumn for a column.\"\"\" if col not in self . store . columns : raise KeyError ( f \"Column { col } does not exist\" ) return byteBars . LazyColumn ( self . store , col ) def iloc ( self , row_slice ): \"\"\"Return rows according to integer-location-based slicing.\"\"\" if isinstance ( row_slice , int ): return pd . DataFrame ([ self . store . retrieve_row ( row_slice )], columns = self . store . columns ) elif isinstance ( row_slice , slice ): start , stop , step = row_slice . indices ( len ( self . store )) return self . store . retrieve_rows ( start , stop ) . iloc [:: step ] elif isinstance ( row_slice , list ) or isinstance ( row_slice , np . ndarray ): rows = [ self . store . retrieve_row ( i ) for i in row_slice ] return pd . DataFrame ( rows , columns = self . store . columns ) else : raise TypeError ( \"Unsupported index type for iloc\" ) def head ( self , n = 5 ): \"\"\"Return first n rows.\"\"\" return self . iloc ( slice ( 0 , n )) def tail ( self , n = 5 ): \"\"\"Return last n rows.\"\"\" return self . iloc ( slice ( len ( self . store ) - n , len ( self . store ))) def to_numpy ( self , row_slice = None ): \"\"\"Return entire view as NumPy array.\"\"\" df = self . iloc ( row_slice if row_slice else slice ( 0 , len ( self . store ))) return df . to_numpy () __getitem__ ( col ) Return LazyColumn for a column. Source code in ironBars/byteBars.py 338 339 340 341 342 def __getitem__ ( self , col ): \"\"\"Return LazyColumn for a column.\"\"\" if col not in self . store . columns : raise KeyError ( f \"Column { col } does not exist\" ) return byteBars . LazyColumn ( self . store , col ) head ( n = 5 ) Return first n rows. Source code in ironBars/byteBars.py 357 358 359 def head ( self , n = 5 ): \"\"\"Return first n rows.\"\"\" return self . iloc ( slice ( 0 , n )) iloc ( row_slice ) Return rows according to integer-location-based slicing. Source code in ironBars/byteBars.py 344 345 346 347 348 349 350 351 352 353 354 355 def iloc ( self , row_slice ): \"\"\"Return rows according to integer-location-based slicing.\"\"\" if isinstance ( row_slice , int ): return pd . DataFrame ([ self . store . retrieve_row ( row_slice )], columns = self . store . columns ) elif isinstance ( row_slice , slice ): start , stop , step = row_slice . indices ( len ( self . store )) return self . store . retrieve_rows ( start , stop ) . iloc [:: step ] elif isinstance ( row_slice , list ) or isinstance ( row_slice , np . ndarray ): rows = [ self . store . retrieve_row ( i ) for i in row_slice ] return pd . DataFrame ( rows , columns = self . store . columns ) else : raise TypeError ( \"Unsupported index type for iloc\" ) tail ( n = 5 ) Return last n rows. Source code in ironBars/byteBars.py 361 362 363 def tail ( self , n = 5 ): \"\"\"Return last n rows.\"\"\" return self . iloc ( slice ( len ( self . store ) - n , len ( self . store ))) to_numpy ( row_slice = None ) Return entire view as NumPy array. Source code in ironBars/byteBars.py 365 366 367 368 def to_numpy ( self , row_slice = None ): \"\"\"Return entire view as NumPy array.\"\"\" df = self . iloc ( row_slice if row_slice else slice ( 0 , len ( self . store ))) return df . to_numpy () __init__ ( data_file , index_file , block_size = 100 , load_existing = True ) Initialize the LazyCompressedDataFrame. Parameters: data_file : str Path to the binary data file. index_file : str Path to the index file storing block metadata. block_size : int Number of rows per block (for compression). load_existing : bool Whether to load existing files if present. Source code in ironBars/byteBars.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , data_file , index_file , block_size = 100 , load_existing = True ): \"\"\" Initialize the LazyCompressedDataFrame. Parameters: ----------- data_file : str Path to the binary data file. index_file : str Path to the index file storing block metadata. block_size : int Number of rows per block (for compression). load_existing : bool Whether to load existing files if present. \"\"\" self . data_file = data_file self . index_file = index_file self . block_size = block_size self . index = [] self . pending_block = [] self . _last_block_idx = None self . _last_block_data = None self . columns = None # Initialize files if they do not exist if not os . path . exists ( self . data_file ): with open ( self . data_file , \"wb\" ) as f : f . write ( b \"0\" * self . HASH_SIZE ) if not os . path . exists ( self . index_file ): with open ( self . index_file , \"wb\" ) as f : f . write ( b \"0\" * self . HASH_SIZE ) if load_existing : self . load_existing_files () __len__ () Return total number of rows (including pending rows). Source code in ironBars/byteBars.py 377 378 379 380 381 def __len__ ( self ): \"\"\"Return total number of rows (including pending rows).\"\"\" total = sum ( num for _ , _ , num in self . index ) total += len ( self . pending_block ) return total add_dataframe ( df ) Add a Pandas DataFrame to the store. Parameters: df : pd.DataFrame DataFrame to append. Source code in ironBars/byteBars.py 159 160 161 162 163 164 165 166 167 168 169 170 171 def add_dataframe ( self , df ): \"\"\" Add a Pandas DataFrame to the store. Parameters: ----------- df : pd.DataFrame DataFrame to append. \"\"\" if self . columns is None : self . columns = df . columns . tolist () for _ , row in df . iterrows (): self . add_row ( row ) add_row ( row ) Add a single row to the pending block. Parameters: row : dict or pd.Series Row data to append. Columns are inferred from the first row. Source code in ironBars/byteBars.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def add_row ( self , row ): \"\"\" Add a single row to the pending block. Parameters: ----------- row : dict or pd.Series Row data to append. Columns are inferred from the first row. \"\"\" if isinstance ( row , pd . Series ): row = row . to_dict () elif not isinstance ( row , dict ): raise TypeError ( \"Row must be dict or pd.Series\" ) if self . columns is None : self . columns = list ( row . keys ()) self . pending_block . append ( row ) if len ( self . pending_block ) >= self . block_size : self . _flush_block () flush () Flush pending rows to disk. Source code in ironBars/byteBars.py 373 374 375 def flush ( self ): \"\"\"Flush pending rows to disk.\"\"\" self . _flush_block () load_existing_files () Load index and column information from existing files if present. Validates files first. Source code in ironBars/byteBars.py 124 125 126 127 128 129 130 131 132 133 134 135 def load_existing_files ( self ): \"\"\" Load index and column information from existing files if present. Validates files first. \"\"\" if os . path . getsize ( self . data_file ) > self . HASH_SIZE and os . path . getsize ( self . index_file ) > self . HASH_SIZE : self . validate_files () with open ( self . index_file , \"rb\" ) as f : f . seek ( self . HASH_SIZE ) self . index = pickle . load ( f ) if self . index : self . columns = self . _load_block ( 0 )[ 0 ] . keys () retrieve_block ( block_idx , as_dataframe = True ) Retrieve an entire block. Parameters: block_idx : int Block index. as_dataframe : bool Return as pd.DataFrame if True, else list of dicts. Returns: pd.DataFrame or list of dict Source code in ironBars/byteBars.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def retrieve_block ( self , block_idx , as_dataframe = True ): \"\"\" Retrieve an entire block. Parameters: ----------- block_idx : int Block index. as_dataframe : bool Return as pd.DataFrame if True, else list of dicts. Returns: -------- pd.DataFrame or list of dict \"\"\" block = self . _load_block ( block_idx ) if as_dataframe : return pd . DataFrame ( block , columns = self . columns ) return block retrieve_row ( idx ) Retrieve a single row by global index. Parameters: idx : int Row index. Returns: dict : Row data as a dictionary. Source code in ironBars/byteBars.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def retrieve_row ( self , idx ): \"\"\" Retrieve a single row by global index. Parameters: ----------- idx : int Row index. Returns: -------- dict : Row data as a dictionary. \"\"\" running_total = 0 for block_idx , ( _ , _ , num_entries ) in enumerate ( self . index ): if running_total + num_entries > idx : return self . _load_block ( block_idx )[ idx - running_total ] running_total += num_entries pending_idx = idx - running_total if 0 <= pending_idx < len ( self . pending_block ): return self . pending_block [ pending_idx ] raise IndexError ( \"Index out of range\" ) retrieve_rows ( start , end ) Retrieve rows from start to end. Parameters: start : int end : int Returns: pd.DataFrame : Rows in range [start, end) Source code in ironBars/byteBars.py 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 def retrieve_rows ( self , start , end ): \"\"\" Retrieve rows from start to end. Parameters: ----------- start : int end : int Returns: -------- pd.DataFrame : Rows in range [start, end) \"\"\" rows = [ self . retrieve_row ( i ) for i in range ( start , end )] return pd . DataFrame ( rows , columns = self . columns ) validate_files () Validate that data and index files match each other using hashes. Raises ValueError if inconsistency is detected. Source code in ironBars/byteBars.py 107 108 109 110 111 112 113 114 115 116 117 118 119 def validate_files ( self ): \"\"\" Validate that data and index files match each other using hashes. Raises ValueError if inconsistency is detected. \"\"\" data_hash_in_header = self . _read_header ( self . data_file ) index_hash_in_header = self . _read_header ( self . index_file ) current_data_hash = self . _compute_hash ( self . data_file ) current_index_hash = self . _compute_hash ( self . index_file ) if data_hash_in_header != current_index_hash : raise ValueError ( \"Data file header does not match current index file!\" ) if index_hash_in_header != current_data_hash : raise ValueError ( \"Index file header does not match current data file!\" ) Disk-backed, compressed, lazy-loading DataFrame-like storage. Supports row-wise, block-wise, and column-wise access. Stores Pandas DataFrames, Series, lists, and dictionaries efficiently. Provides SHA-256 integrity verification for data and index files. Full DataFrame-like interface via .df with .iloc , .head() , .tail() , and .to_numpy() .","title":"Documentation"},{"location":"#documentation","text":"","title":"Documentation"},{"location":"#ironbars","text":"Convert a Google Sheets, GitHub, or direct CSV link to a CSV-ready URL and optionally load as DataFrame(s). Parameters: Name Type Description Default url str | list URL(s) of CSV file(s). required as_df bool If True, return pandas DataFrame(s) instead of URL(s). False max_workers int | None Max threads for parallel downloading (only for list of URLs). None Returns: Type Description str | list | pd.DataFrame | list[pd.DataFrame] Source code in ironBars/ironSheets.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def gsheet_load ( url , as_df = False , max_workers = None ): \"\"\" Convert a Google Sheets, GitHub, or direct CSV link to a CSV-ready URL and optionally load as DataFrame(s). Parameters: url (str | list): URL(s) of CSV file(s). as_df (bool): If True, return pandas DataFrame(s) instead of URL(s). max_workers (int | None): Max threads for parallel downloading (only for list of URLs). Returns: str | list | pd.DataFrame | list[pd.DataFrame] \"\"\" def convert_url ( single_url ): parsed = urlparse ( single_url ) netloc = parsed . netloc . lower () path = parsed . path . lower () # Google Sheets if \"docs.google.com\" in netloc and \"/spreadsheets/\" in path : if \"/edit\" in single_url : return single_url . split ( \"/edit\" )[ 0 ] + \"/gviz/tq?tqx=out:csv\" else : return single_url # GitHub elif \"github.com\" in netloc : if \"/blob/\" in single_url : return single_url . replace ( \"github.com\" , \"raw.githubusercontent.com\" ) . replace ( \"/blob/\" , \"/\" ) return single_url # Already raw URL # Direct CSV link else : return single_url def fetch_csv ( single_url ): csv_url = convert_url ( single_url ) try : return pd . read_csv ( csv_url ) except Exception as e : raise RuntimeError ( f \"Failed to read CSV from { single_url } : { e } \" ) # Handle single URL vs list if isinstance ( url , list ): if as_df : with ThreadPoolExecutor ( max_workers = max_workers ) as executor : return list ( executor . map ( fetch_csv , url )) else : return [ convert_url ( u ) for u in url ] else : converted = convert_url ( url ) return pd . read_csv ( converted ) if as_df else converted Save one or more DataFrames as CSV files. Parameters: Name Type Description Default data_frames DataFrame | list [ DataFrame ] Single dataframe or list of dataframes to save. required auto_name bool If True, generates filenames automatically using name_series + index (only for multiple frames). True name_series str | list Base name (str) or list of names (required if auto_name=False and multiple frames). 'Sheet' save_dir str Directory to save CSV files. Defaults to current directory. '.' filename str | None Filename for single dataframe. Required if saving a single DataFrame. None Source code in ironBars/ironSheets.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def gsheet_save ( data_frames , auto_name = True , name_series = \"Sheet\" , save_dir = \".\" , filename = None ): \"\"\" Save one or more DataFrames as CSV files. Parameters: data_frames (pd.DataFrame | list[pd.DataFrame]): Single dataframe or list of dataframes to save. auto_name (bool): If True, generates filenames automatically using name_series + index (only for multiple frames). name_series (str | list): Base name (str) or list of names (required if auto_name=False and multiple frames). save_dir (str): Directory to save CSV files. Defaults to current directory. filename (str | None): Filename for single dataframe. Required if saving a single DataFrame. \"\"\" # Normalize input to list single_input = False if isinstance ( data_frames , pd . DataFrame ): data_frames = [ data_frames ] single_input = True assert data_frames , \"DATA FRAMES must not be empty!\" # Ensure absolute save directory path save_dir = os . path . abspath ( save_dir ) os . makedirs ( save_dir , exist_ok = True ) # Handle single dataframe case if single_input : assert filename , \"Must provide 'filename' when saving a single DataFrame\" if not filename . lower () . endswith ( \".csv\" ): filename += \".csv\" fpath = os . path . join ( save_dir , filename ) data_frames [ 0 ] . to_csv ( fpath , index = False ) return # Nothing to return # Handle multiple dataframes if auto_name : if isinstance ( name_series , list ): raise RuntimeError ( \"Auto Name -> True but name_series is a list! Provide a string base name instead.\" ) else : if not isinstance ( name_series , list ): raise RuntimeError ( \"Auto Name -> False. Provide a list of filenames as name_series.\" ) if len ( name_series ) < len ( data_frames ): raise ValueError ( \"Number of filenames provided is less than number of data frames.\" ) for idx , frame in enumerate ( data_frames ): if auto_name : fname = f \" { name_series }{ idx } .csv\" else : fname = f \" { name_series [ idx ] } .csv\" fpath = os . path . join ( save_dir , fname ) frame . to_csv ( fpath , index = False ) Fill NaN values in a DataFrame column or all numeric columns using either 'perlin' noise or 'linear' regression. df : pandas.DataFrame Input DataFrame containing numeric columns. column_name : str, optional Column to fill. If None, all numeric columns will be filled. method : str, default \"perlin\" Filling method: \"perlin\" for Perlin noise, \"linear\" for linear regression. seed : int, default 0 Seed or starting index for Perlin noise generation. scale_factor : float, default 0.1 Step size for Perlin noise generation. lim_min : float, default 0 Minimum limit for filled values. lim_max : float, default 100 Maximum limit for filled values. pandas.DataFrame A copy of the original DataFrame with NaNs filled, respecting user-defined limits. Raises: - TypeError: If the specified column is not numeric. - ValueError: If the method is not \"perlin\" or \"linear\". Source code in ironBars/ironSheets.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def fill_nans ( df , column_name = None , method = \"perlin\" , seed = 0 , scale_factor = 0.1 , lim_min = 0 , lim_max = 100 ): \"\"\" Fill NaN values in a DataFrame column or all numeric columns using either 'perlin' noise or 'linear' regression. Parameters: - df : pandas.DataFrame Input DataFrame containing numeric columns. - column_name : str, optional Column to fill. If None, all numeric columns will be filled. - method : str, default \"perlin\" Filling method: \"perlin\" for Perlin noise, \"linear\" for linear regression. - seed : int, default 0 Seed or starting index for Perlin noise generation. - scale_factor : float, default 0.1 Step size for Perlin noise generation. - lim_min : float, default 0 Minimum limit for filled values. - lim_max : float, default 100 Maximum limit for filled values. Returns: - pandas.DataFrame A copy of the original DataFrame with NaNs filled, respecting user-defined limits. Raises: - TypeError: If the specified column is not numeric. - ValueError: If the method is not \"perlin\" or \"linear\". \"\"\" df_copy = df . copy () if column_name is not None : if not np . issubdtype ( df_copy [ column_name ] . dtype , np . number ): raise TypeError ( f \"Column ' { column_name } ' must be numeric.\" ) cols_to_fill = [ column_name ] else : cols_to_fill = df_copy . select_dtypes ( include = [ np . number ]) . columns . tolist () for col in cols_to_fill : arr = df_copy [ col ] . to_numpy ( dtype = float ) nan_mask = np . isnan ( arr ) if not nan_mask . any (): continue if method == \"perlin\" : mean_val = np . nanmean ( arr ) std_val = np . nanstd ( arr ) indices = np . arange ( len ( arr )) noise_vals = np . array ([ pnoise1 (( i + seed ) * scale_factor ) for i in indices ]) noise_scaled = noise_vals * 2 * std_val + mean_val noise_scaled = np . clip ( noise_scaled , lim_min , lim_max ) arr [ nan_mask ] = noise_scaled [ nan_mask ] elif method == \"linear\" : indices_all = np . arange ( len ( arr )) . reshape ( - 1 , 1 ) X_train = indices_all [ ~ nan_mask ] y_train = arr [ ~ nan_mask ] X_pred = indices_all [ nan_mask ] model = LinearRegression () model . fit ( X_train , y_train ) arr [ nan_mask ] = model . predict ( X_pred ) arr = np . clip ( arr , lim_min , lim_max ) else : raise ValueError ( \"Method must be 'perlin' or 'linear'\" ) df_copy [ col ] = arr return df_copy","title":"ironBars"},{"location":"#bytebars","text":"A disk-backed, compressed, lazy-loading DataFrame-like storage system. Supports row-wise and column-wise access, vectorized operations, appending new data, and SHA-256 integrity verification. Source code in ironBars/byteBars.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 class byteBars : \"\"\" A disk-backed, compressed, lazy-loading DataFrame-like storage system. Supports row-wise and column-wise access, vectorized operations, appending new data, and SHA-256 integrity verification. \"\"\" HASH_SIZE = 64 # SHA-256 hex string length def __init__ ( self , data_file , index_file , block_size = 100 , load_existing = True ): \"\"\" Initialize the LazyCompressedDataFrame. Parameters: ----------- data_file : str Path to the binary data file. index_file : str Path to the index file storing block metadata. block_size : int Number of rows per block (for compression). load_existing : bool Whether to load existing files if present. \"\"\" self . data_file = data_file self . index_file = index_file self . block_size = block_size self . index = [] self . pending_block = [] self . _last_block_idx = None self . _last_block_data = None self . columns = None # Initialize files if they do not exist if not os . path . exists ( self . data_file ): with open ( self . data_file , \"wb\" ) as f : f . write ( b \"0\" * self . HASH_SIZE ) if not os . path . exists ( self . index_file ): with open ( self . index_file , \"wb\" ) as f : f . write ( b \"0\" * self . HASH_SIZE ) if load_existing : self . load_existing_files () # ------------------------------ # Hash utilities # ------------------------------ def _compute_hash ( self , file_path ): \"\"\" Compute SHA-256 hash of the file, skipping the header. Parameters: ----------- file_path : str Path to the file. Returns: -------- str : SHA-256 hex digest. \"\"\" hasher = hashlib . sha256 () with open ( file_path , \"rb\" ) as f : f . seek ( self . HASH_SIZE ) for chunk in iter ( lambda : f . read ( 8192 ), b \"\" ): hasher . update ( chunk ) return hasher . hexdigest () def _write_header ( self , file_path , hash_str ): \"\"\" Write a hash string to the header of a file. Parameters: ----------- file_path : str Path to the file. hash_str : str SHA-256 hash string to write. \"\"\" with open ( file_path , \"r+b\" ) as f : f . seek ( 0 ) f . write ( hash_str . encode ( \"utf-8\" )) def _read_header ( self , file_path ): \"\"\" Read the SHA-256 hash from the header of a file. Parameters: ----------- file_path : str Path to the file. Returns: -------- str : Hash string stored in header. \"\"\" with open ( file_path , \"rb\" ) as f : f . seek ( 0 ) return f . read ( self . HASH_SIZE ) . decode ( \"utf-8\" ) def validate_files ( self ): \"\"\" Validate that data and index files match each other using hashes. Raises ValueError if inconsistency is detected. \"\"\" data_hash_in_header = self . _read_header ( self . data_file ) index_hash_in_header = self . _read_header ( self . index_file ) current_data_hash = self . _compute_hash ( self . data_file ) current_index_hash = self . _compute_hash ( self . index_file ) if data_hash_in_header != current_index_hash : raise ValueError ( \"Data file header does not match current index file!\" ) if index_hash_in_header != current_data_hash : raise ValueError ( \"Index file header does not match current data file!\" ) # ------------------------------ # Load existing files # ------------------------------ def load_existing_files ( self ): \"\"\" Load index and column information from existing files if present. Validates files first. \"\"\" if os . path . getsize ( self . data_file ) > self . HASH_SIZE and os . path . getsize ( self . index_file ) > self . HASH_SIZE : self . validate_files () with open ( self . index_file , \"rb\" ) as f : f . seek ( self . HASH_SIZE ) self . index = pickle . load ( f ) if self . index : self . columns = self . _load_block ( 0 )[ 0 ] . keys () # ------------------------------ # Core methods # ------------------------------ def add_row ( self , row ): \"\"\" Add a single row to the pending block. Parameters: ----------- row : dict or pd.Series Row data to append. Columns are inferred from the first row. \"\"\" if isinstance ( row , pd . Series ): row = row . to_dict () elif not isinstance ( row , dict ): raise TypeError ( \"Row must be dict or pd.Series\" ) if self . columns is None : self . columns = list ( row . keys ()) self . pending_block . append ( row ) if len ( self . pending_block ) >= self . block_size : self . _flush_block () def add_dataframe ( self , df ): \"\"\" Add a Pandas DataFrame to the store. Parameters: ----------- df : pd.DataFrame DataFrame to append. \"\"\" if self . columns is None : self . columns = df . columns . tolist () for _ , row in df . iterrows (): self . add_row ( row ) def _flush_block ( self ): \"\"\" Compress and write the pending block to disk, update index and headers. \"\"\" if not self . pending_block : return serialized = pickle . dumps ( self . pending_block ) compressed = zlib . compress ( serialized , level = 9 ) with open ( self . data_file , \"ab\" ) as f : offset = f . tell () f . write ( compressed ) length = len ( compressed ) self . index . append (( offset , length , len ( self . pending_block ))) self . pending_block = [] self . _save_index () self . _update_headers () self . _last_block_idx = None self . _last_block_data = None def _save_index ( self ): \"\"\"Serialize and save the index to the index file.\"\"\" with open ( self . index_file , \"r+b\" ) as f : f . seek ( self . HASH_SIZE ) pickle . dump ( self . index , f ) def _update_headers ( self ): \"\"\"Update the SHA-256 headers for integrity check.\"\"\" data_hash = self . _compute_hash ( self . index_file ) index_hash = self . _compute_hash ( self . data_file ) self . _write_header ( self . data_file , data_hash ) self . _write_header ( self . index_file , index_hash ) def _load_block ( self , block_idx ): \"\"\" Load a block from disk (decompress) or return cached. Parameters: ----------- block_idx : int Index of block to load. Returns: -------- list : List of row dictionaries in the block. \"\"\" if self . _last_block_idx == block_idx : return self . _last_block_data block_offset , block_length , _ = self . index [ block_idx ] with open ( self . data_file , \"rb\" ) as f : f . seek ( block_offset ) compressed = f . read ( block_length ) block_data = pickle . loads ( zlib . decompress ( compressed )) self . _last_block_idx = block_idx self . _last_block_data = block_data return block_data # ------------------------------ # Retrieval # ------------------------------ def retrieve_row ( self , idx ): \"\"\" Retrieve a single row by global index. Parameters: ----------- idx : int Row index. Returns: -------- dict : Row data as a dictionary. \"\"\" running_total = 0 for block_idx , ( _ , _ , num_entries ) in enumerate ( self . index ): if running_total + num_entries > idx : return self . _load_block ( block_idx )[ idx - running_total ] running_total += num_entries pending_idx = idx - running_total if 0 <= pending_idx < len ( self . pending_block ): return self . pending_block [ pending_idx ] raise IndexError ( \"Index out of range\" ) def retrieve_block ( self , block_idx , as_dataframe = True ): \"\"\" Retrieve an entire block. Parameters: ----------- block_idx : int Block index. as_dataframe : bool Return as pd.DataFrame if True, else list of dicts. Returns: -------- pd.DataFrame or list of dict \"\"\" block = self . _load_block ( block_idx ) if as_dataframe : return pd . DataFrame ( block , columns = self . columns ) return block def retrieve_rows ( self , start , end ): \"\"\" Retrieve rows from start to end. Parameters: ----------- start : int end : int Returns: -------- pd.DataFrame : Rows in range [start, end) \"\"\" rows = [ self . retrieve_row ( i ) for i in range ( start , end )] return pd . DataFrame ( rows , columns = self . columns ) # ------------------------------ # Lazy DataFrame-like interface # ------------------------------ @property def df ( self ): \"\"\"Provides a lazy Pandas-like view of the dataset.\"\"\" return self . LazyView ( self ) class LazyColumn : \"\"\"Represents a lazy-access column for vectorized operations.\"\"\" def __init__ ( self , store , column ): self . store = store self . column = column def to_numpy ( self , row_slice = None ): \"\"\" Return column data as NumPy array for specified row slice. Parameters: ----------- row_slice : int, slice, or list/array, optional Returns: -------- np.ndarray \"\"\" if row_slice is None : row_slice = slice ( 0 , len ( self . store )) if isinstance ( row_slice , int ): return np . array ([ self . store . retrieve_row ( row_slice )[ self . column ]]) elif isinstance ( row_slice , slice ): start , stop , step = row_slice . indices ( len ( self . store )) data = [ self . store . retrieve_row ( i )[ self . column ] for i in range ( start , stop , step )] return np . array ( data ) elif isinstance ( row_slice , list ) or isinstance ( row_slice , np . ndarray ): data = [ self . store . retrieve_row ( i )[ self . column ] for i in row_slice ] return np . array ( data ) else : raise TypeError ( \"Unsupported index type\" ) class LazyView : \"\"\"Provides lazy DataFrame-like access.\"\"\" def __init__ ( self , store ): self . store = store def __getitem__ ( self , col ): \"\"\"Return LazyColumn for a column.\"\"\" if col not in self . store . columns : raise KeyError ( f \"Column { col } does not exist\" ) return byteBars . LazyColumn ( self . store , col ) def iloc ( self , row_slice ): \"\"\"Return rows according to integer-location-based slicing.\"\"\" if isinstance ( row_slice , int ): return pd . DataFrame ([ self . store . retrieve_row ( row_slice )], columns = self . store . columns ) elif isinstance ( row_slice , slice ): start , stop , step = row_slice . indices ( len ( self . store )) return self . store . retrieve_rows ( start , stop ) . iloc [:: step ] elif isinstance ( row_slice , list ) or isinstance ( row_slice , np . ndarray ): rows = [ self . store . retrieve_row ( i ) for i in row_slice ] return pd . DataFrame ( rows , columns = self . store . columns ) else : raise TypeError ( \"Unsupported index type for iloc\" ) def head ( self , n = 5 ): \"\"\"Return first n rows.\"\"\" return self . iloc ( slice ( 0 , n )) def tail ( self , n = 5 ): \"\"\"Return last n rows.\"\"\" return self . iloc ( slice ( len ( self . store ) - n , len ( self . store ))) def to_numpy ( self , row_slice = None ): \"\"\"Return entire view as NumPy array.\"\"\" df = self . iloc ( row_slice if row_slice else slice ( 0 , len ( self . store ))) return df . to_numpy () # ------------------------------ # Utilities # ------------------------------ def flush ( self ): \"\"\"Flush pending rows to disk.\"\"\" self . _flush_block () def __len__ ( self ): \"\"\"Return total number of rows (including pending rows).\"\"\" total = sum ( num for _ , _ , num in self . index ) total += len ( self . pending_block ) return total","title":"byteBars"},{"location":"#ironBars.byteBars.byteBars.df","text":"Provides a lazy Pandas-like view of the dataset.","title":"df"},{"location":"#ironBars.byteBars.byteBars.LazyColumn","text":"Represents a lazy-access column for vectorized operations. Source code in ironBars/byteBars.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 class LazyColumn : \"\"\"Represents a lazy-access column for vectorized operations.\"\"\" def __init__ ( self , store , column ): self . store = store self . column = column def to_numpy ( self , row_slice = None ): \"\"\" Return column data as NumPy array for specified row slice. Parameters: ----------- row_slice : int, slice, or list/array, optional Returns: -------- np.ndarray \"\"\" if row_slice is None : row_slice = slice ( 0 , len ( self . store )) if isinstance ( row_slice , int ): return np . array ([ self . store . retrieve_row ( row_slice )[ self . column ]]) elif isinstance ( row_slice , slice ): start , stop , step = row_slice . indices ( len ( self . store )) data = [ self . store . retrieve_row ( i )[ self . column ] for i in range ( start , stop , step )] return np . array ( data ) elif isinstance ( row_slice , list ) or isinstance ( row_slice , np . ndarray ): data = [ self . store . retrieve_row ( i )[ self . column ] for i in row_slice ] return np . array ( data ) else : raise TypeError ( \"Unsupported index type\" )","title":"LazyColumn"},{"location":"#ironBars.byteBars.byteBars.LazyColumn.to_numpy","text":"Return column data as NumPy array for specified row slice.","title":"to_numpy"},{"location":"#ironBars.byteBars.byteBars.LazyColumn.to_numpy--parameters","text":"row_slice : int, slice, or list/array, optional","title":"Parameters:"},{"location":"#ironBars.byteBars.byteBars.LazyColumn.to_numpy--returns","text":"np.ndarray Source code in ironBars/byteBars.py 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 def to_numpy ( self , row_slice = None ): \"\"\" Return column data as NumPy array for specified row slice. Parameters: ----------- row_slice : int, slice, or list/array, optional Returns: -------- np.ndarray \"\"\" if row_slice is None : row_slice = slice ( 0 , len ( self . store )) if isinstance ( row_slice , int ): return np . array ([ self . store . retrieve_row ( row_slice )[ self . column ]]) elif isinstance ( row_slice , slice ): start , stop , step = row_slice . indices ( len ( self . store )) data = [ self . store . retrieve_row ( i )[ self . column ] for i in range ( start , stop , step )] return np . array ( data ) elif isinstance ( row_slice , list ) or isinstance ( row_slice , np . ndarray ): data = [ self . store . retrieve_row ( i )[ self . column ] for i in row_slice ] return np . array ( data ) else : raise TypeError ( \"Unsupported index type\" )","title":"Returns:"},{"location":"#ironBars.byteBars.byteBars.LazyView","text":"Provides lazy DataFrame-like access. Source code in ironBars/byteBars.py 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 class LazyView : \"\"\"Provides lazy DataFrame-like access.\"\"\" def __init__ ( self , store ): self . store = store def __getitem__ ( self , col ): \"\"\"Return LazyColumn for a column.\"\"\" if col not in self . store . columns : raise KeyError ( f \"Column { col } does not exist\" ) return byteBars . LazyColumn ( self . store , col ) def iloc ( self , row_slice ): \"\"\"Return rows according to integer-location-based slicing.\"\"\" if isinstance ( row_slice , int ): return pd . DataFrame ([ self . store . retrieve_row ( row_slice )], columns = self . store . columns ) elif isinstance ( row_slice , slice ): start , stop , step = row_slice . indices ( len ( self . store )) return self . store . retrieve_rows ( start , stop ) . iloc [:: step ] elif isinstance ( row_slice , list ) or isinstance ( row_slice , np . ndarray ): rows = [ self . store . retrieve_row ( i ) for i in row_slice ] return pd . DataFrame ( rows , columns = self . store . columns ) else : raise TypeError ( \"Unsupported index type for iloc\" ) def head ( self , n = 5 ): \"\"\"Return first n rows.\"\"\" return self . iloc ( slice ( 0 , n )) def tail ( self , n = 5 ): \"\"\"Return last n rows.\"\"\" return self . iloc ( slice ( len ( self . store ) - n , len ( self . store ))) def to_numpy ( self , row_slice = None ): \"\"\"Return entire view as NumPy array.\"\"\" df = self . iloc ( row_slice if row_slice else slice ( 0 , len ( self . store ))) return df . to_numpy ()","title":"LazyView"},{"location":"#ironBars.byteBars.byteBars.LazyView.__getitem__","text":"Return LazyColumn for a column. Source code in ironBars/byteBars.py 338 339 340 341 342 def __getitem__ ( self , col ): \"\"\"Return LazyColumn for a column.\"\"\" if col not in self . store . columns : raise KeyError ( f \"Column { col } does not exist\" ) return byteBars . LazyColumn ( self . store , col )","title":"__getitem__"},{"location":"#ironBars.byteBars.byteBars.LazyView.head","text":"Return first n rows. Source code in ironBars/byteBars.py 357 358 359 def head ( self , n = 5 ): \"\"\"Return first n rows.\"\"\" return self . iloc ( slice ( 0 , n ))","title":"head"},{"location":"#ironBars.byteBars.byteBars.LazyView.iloc","text":"Return rows according to integer-location-based slicing. Source code in ironBars/byteBars.py 344 345 346 347 348 349 350 351 352 353 354 355 def iloc ( self , row_slice ): \"\"\"Return rows according to integer-location-based slicing.\"\"\" if isinstance ( row_slice , int ): return pd . DataFrame ([ self . store . retrieve_row ( row_slice )], columns = self . store . columns ) elif isinstance ( row_slice , slice ): start , stop , step = row_slice . indices ( len ( self . store )) return self . store . retrieve_rows ( start , stop ) . iloc [:: step ] elif isinstance ( row_slice , list ) or isinstance ( row_slice , np . ndarray ): rows = [ self . store . retrieve_row ( i ) for i in row_slice ] return pd . DataFrame ( rows , columns = self . store . columns ) else : raise TypeError ( \"Unsupported index type for iloc\" )","title":"iloc"},{"location":"#ironBars.byteBars.byteBars.LazyView.tail","text":"Return last n rows. Source code in ironBars/byteBars.py 361 362 363 def tail ( self , n = 5 ): \"\"\"Return last n rows.\"\"\" return self . iloc ( slice ( len ( self . store ) - n , len ( self . store )))","title":"tail"},{"location":"#ironBars.byteBars.byteBars.LazyView.to_numpy","text":"Return entire view as NumPy array. Source code in ironBars/byteBars.py 365 366 367 368 def to_numpy ( self , row_slice = None ): \"\"\"Return entire view as NumPy array.\"\"\" df = self . iloc ( row_slice if row_slice else slice ( 0 , len ( self . store ))) return df . to_numpy ()","title":"to_numpy"},{"location":"#ironBars.byteBars.byteBars.__init__","text":"Initialize the LazyCompressedDataFrame.","title":"__init__"},{"location":"#ironBars.byteBars.byteBars.__init__--parameters","text":"data_file : str Path to the binary data file. index_file : str Path to the index file storing block metadata. block_size : int Number of rows per block (for compression). load_existing : bool Whether to load existing files if present. Source code in ironBars/byteBars.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , data_file , index_file , block_size = 100 , load_existing = True ): \"\"\" Initialize the LazyCompressedDataFrame. Parameters: ----------- data_file : str Path to the binary data file. index_file : str Path to the index file storing block metadata. block_size : int Number of rows per block (for compression). load_existing : bool Whether to load existing files if present. \"\"\" self . data_file = data_file self . index_file = index_file self . block_size = block_size self . index = [] self . pending_block = [] self . _last_block_idx = None self . _last_block_data = None self . columns = None # Initialize files if they do not exist if not os . path . exists ( self . data_file ): with open ( self . data_file , \"wb\" ) as f : f . write ( b \"0\" * self . HASH_SIZE ) if not os . path . exists ( self . index_file ): with open ( self . index_file , \"wb\" ) as f : f . write ( b \"0\" * self . HASH_SIZE ) if load_existing : self . load_existing_files ()","title":"Parameters:"},{"location":"#ironBars.byteBars.byteBars.__len__","text":"Return total number of rows (including pending rows). Source code in ironBars/byteBars.py 377 378 379 380 381 def __len__ ( self ): \"\"\"Return total number of rows (including pending rows).\"\"\" total = sum ( num for _ , _ , num in self . index ) total += len ( self . pending_block ) return total","title":"__len__"},{"location":"#ironBars.byteBars.byteBars.add_dataframe","text":"Add a Pandas DataFrame to the store.","title":"add_dataframe"},{"location":"#ironBars.byteBars.byteBars.add_dataframe--parameters","text":"df : pd.DataFrame DataFrame to append. Source code in ironBars/byteBars.py 159 160 161 162 163 164 165 166 167 168 169 170 171 def add_dataframe ( self , df ): \"\"\" Add a Pandas DataFrame to the store. Parameters: ----------- df : pd.DataFrame DataFrame to append. \"\"\" if self . columns is None : self . columns = df . columns . tolist () for _ , row in df . iterrows (): self . add_row ( row )","title":"Parameters:"},{"location":"#ironBars.byteBars.byteBars.add_row","text":"Add a single row to the pending block.","title":"add_row"},{"location":"#ironBars.byteBars.byteBars.add_row--parameters","text":"row : dict or pd.Series Row data to append. Columns are inferred from the first row. Source code in ironBars/byteBars.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def add_row ( self , row ): \"\"\" Add a single row to the pending block. Parameters: ----------- row : dict or pd.Series Row data to append. Columns are inferred from the first row. \"\"\" if isinstance ( row , pd . Series ): row = row . to_dict () elif not isinstance ( row , dict ): raise TypeError ( \"Row must be dict or pd.Series\" ) if self . columns is None : self . columns = list ( row . keys ()) self . pending_block . append ( row ) if len ( self . pending_block ) >= self . block_size : self . _flush_block ()","title":"Parameters:"},{"location":"#ironBars.byteBars.byteBars.flush","text":"Flush pending rows to disk. Source code in ironBars/byteBars.py 373 374 375 def flush ( self ): \"\"\"Flush pending rows to disk.\"\"\" self . _flush_block ()","title":"flush"},{"location":"#ironBars.byteBars.byteBars.load_existing_files","text":"Load index and column information from existing files if present. Validates files first. Source code in ironBars/byteBars.py 124 125 126 127 128 129 130 131 132 133 134 135 def load_existing_files ( self ): \"\"\" Load index and column information from existing files if present. Validates files first. \"\"\" if os . path . getsize ( self . data_file ) > self . HASH_SIZE and os . path . getsize ( self . index_file ) > self . HASH_SIZE : self . validate_files () with open ( self . index_file , \"rb\" ) as f : f . seek ( self . HASH_SIZE ) self . index = pickle . load ( f ) if self . index : self . columns = self . _load_block ( 0 )[ 0 ] . keys ()","title":"load_existing_files"},{"location":"#ironBars.byteBars.byteBars.retrieve_block","text":"Retrieve an entire block.","title":"retrieve_block"},{"location":"#ironBars.byteBars.byteBars.retrieve_block--parameters","text":"block_idx : int Block index. as_dataframe : bool Return as pd.DataFrame if True, else list of dicts.","title":"Parameters:"},{"location":"#ironBars.byteBars.byteBars.retrieve_block--returns","text":"pd.DataFrame or list of dict Source code in ironBars/byteBars.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def retrieve_block ( self , block_idx , as_dataframe = True ): \"\"\" Retrieve an entire block. Parameters: ----------- block_idx : int Block index. as_dataframe : bool Return as pd.DataFrame if True, else list of dicts. Returns: -------- pd.DataFrame or list of dict \"\"\" block = self . _load_block ( block_idx ) if as_dataframe : return pd . DataFrame ( block , columns = self . columns ) return block","title":"Returns:"},{"location":"#ironBars.byteBars.byteBars.retrieve_row","text":"Retrieve a single row by global index.","title":"retrieve_row"},{"location":"#ironBars.byteBars.byteBars.retrieve_row--parameters","text":"idx : int Row index.","title":"Parameters:"},{"location":"#ironBars.byteBars.byteBars.retrieve_row--returns","text":"dict : Row data as a dictionary. Source code in ironBars/byteBars.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def retrieve_row ( self , idx ): \"\"\" Retrieve a single row by global index. Parameters: ----------- idx : int Row index. Returns: -------- dict : Row data as a dictionary. \"\"\" running_total = 0 for block_idx , ( _ , _ , num_entries ) in enumerate ( self . index ): if running_total + num_entries > idx : return self . _load_block ( block_idx )[ idx - running_total ] running_total += num_entries pending_idx = idx - running_total if 0 <= pending_idx < len ( self . pending_block ): return self . pending_block [ pending_idx ] raise IndexError ( \"Index out of range\" )","title":"Returns:"},{"location":"#ironBars.byteBars.byteBars.retrieve_rows","text":"Retrieve rows from start to end.","title":"retrieve_rows"},{"location":"#ironBars.byteBars.byteBars.retrieve_rows--parameters","text":"start : int end : int","title":"Parameters:"},{"location":"#ironBars.byteBars.byteBars.retrieve_rows--returns","text":"pd.DataFrame : Rows in range [start, end) Source code in ironBars/byteBars.py 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 def retrieve_rows ( self , start , end ): \"\"\" Retrieve rows from start to end. Parameters: ----------- start : int end : int Returns: -------- pd.DataFrame : Rows in range [start, end) \"\"\" rows = [ self . retrieve_row ( i ) for i in range ( start , end )] return pd . DataFrame ( rows , columns = self . columns )","title":"Returns:"},{"location":"#ironBars.byteBars.byteBars.validate_files","text":"Validate that data and index files match each other using hashes. Raises ValueError if inconsistency is detected. Source code in ironBars/byteBars.py 107 108 109 110 111 112 113 114 115 116 117 118 119 def validate_files ( self ): \"\"\" Validate that data and index files match each other using hashes. Raises ValueError if inconsistency is detected. \"\"\" data_hash_in_header = self . _read_header ( self . data_file ) index_hash_in_header = self . _read_header ( self . index_file ) current_data_hash = self . _compute_hash ( self . data_file ) current_index_hash = self . _compute_hash ( self . index_file ) if data_hash_in_header != current_index_hash : raise ValueError ( \"Data file header does not match current index file!\" ) if index_hash_in_header != current_data_hash : raise ValueError ( \"Index file header does not match current data file!\" ) Disk-backed, compressed, lazy-loading DataFrame-like storage. Supports row-wise, block-wise, and column-wise access. Stores Pandas DataFrames, Series, lists, and dictionaries efficiently. Provides SHA-256 integrity verification for data and index files. Full DataFrame-like interface via .df with .iloc , .head() , .tail() , and .to_numpy() .","title":"validate_files"}]}