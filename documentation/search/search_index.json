{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IronBars Documentation gsheet_load Convert a Google Sheets, GitHub, or direct CSV link to a CSV-ready URL and optionally load as DataFrame(s). Parameters: Name Type Description Default url str | list URL(s) of CSV file(s). required as_df bool If True, return pandas DataFrame(s) instead of URL(s). False max_workers int | None Max threads for parallel downloading (only for list of URLs). None Returns: Type Description str | list | pd.DataFrame | list[pd.DataFrame] Source code in ironBars/ironSheets.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def gsheet_load ( url , as_df = False , max_workers = None ): \"\"\" Convert a Google Sheets, GitHub, or direct CSV link to a CSV-ready URL and optionally load as DataFrame(s). Parameters: url (str | list): URL(s) of CSV file(s). as_df (bool): If True, return pandas DataFrame(s) instead of URL(s). max_workers (int | None): Max threads for parallel downloading (only for list of URLs). Returns: str | list | pd.DataFrame | list[pd.DataFrame] \"\"\" def convert_url ( single_url ): parsed = urlparse ( single_url ) netloc = parsed . netloc . lower () path = parsed . path . lower () # Google Sheets if \"docs.google.com\" in netloc and \"/spreadsheets/\" in path : if \"/edit\" in single_url : return single_url . split ( \"/edit\" )[ 0 ] + \"/gviz/tq?tqx=out:csv\" else : return single_url # GitHub elif \"github.com\" in netloc : if \"/blob/\" in single_url : return single_url . replace ( \"github.com\" , \"raw.githubusercontent.com\" ) . replace ( \"/blob/\" , \"/\" ) return single_url # Already raw URL # Direct CSV link else : return single_url def fetch_csv ( single_url ): csv_url = convert_url ( single_url ) try : return pd . read_csv ( csv_url ) except Exception as e : raise RuntimeError ( f \"Failed to read CSV from { single_url } : { e } \" ) # Handle single URL vs list if isinstance ( url , list ): if as_df : with ThreadPoolExecutor ( max_workers = max_workers ) as executor : return list ( executor . map ( fetch_csv , url )) else : return [ convert_url ( u ) for u in url ] else : converted = convert_url ( url ) return pd . read_csv ( converted ) if as_df else converted Load data from a Google Sheet into a pandas DataFrame. gsheet_save Save one or more DataFrames as CSV files. Parameters: Name Type Description Default data_frames DataFrame | list [ DataFrame ] Single dataframe or list of dataframes to save. required auto_name bool If True, generates filenames automatically using name_series + index (only for multiple frames). True name_series str | list Base name (str) or list of names (required if auto_name=False and multiple frames). 'Sheet' save_dir str Directory to save CSV files. Defaults to current directory. '.' filename str | None Filename for single dataframe. Required if saving a single DataFrame. None Source code in ironBars/ironSheets.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def gsheet_save ( data_frames , auto_name = True , name_series = \"Sheet\" , save_dir = \".\" , filename = None ): \"\"\" Save one or more DataFrames as CSV files. Parameters: data_frames (pd.DataFrame | list[pd.DataFrame]): Single dataframe or list of dataframes to save. auto_name (bool): If True, generates filenames automatically using name_series + index (only for multiple frames). name_series (str | list): Base name (str) or list of names (required if auto_name=False and multiple frames). save_dir (str): Directory to save CSV files. Defaults to current directory. filename (str | None): Filename for single dataframe. Required if saving a single DataFrame. \"\"\" # Normalize input to list single_input = False if isinstance ( data_frames , pd . DataFrame ): data_frames = [ data_frames ] single_input = True assert data_frames , \"DATA FRAMES must not be empty!\" # Ensure absolute save directory path save_dir = os . path . abspath ( save_dir ) os . makedirs ( save_dir , exist_ok = True ) # Handle single dataframe case if single_input : assert filename , \"Must provide 'filename' when saving a single DataFrame\" if not filename . lower () . endswith ( \".csv\" ): filename += \".csv\" fpath = os . path . join ( save_dir , filename ) data_frames [ 0 ] . to_csv ( fpath , index = False ) return # Nothing to return # Handle multiple dataframes if auto_name : if isinstance ( name_series , list ): raise RuntimeError ( \"Auto Name -> True but name_series is a list! Provide a string base name instead.\" ) else : if not isinstance ( name_series , list ): raise RuntimeError ( \"Auto Name -> False. Provide a list of filenames as name_series.\" ) if len ( name_series ) < len ( data_frames ): raise ValueError ( \"Number of filenames provided is less than number of data frames.\" ) for idx , frame in enumerate ( data_frames ): if auto_name : fname = f \" { name_series }{ idx } .csv\" else : fname = f \" { name_series [ idx ] } .csv\" fpath = os . path . join ( save_dir , fname ) frame . to_csv ( fpath , index = False ) Save a pandas DataFrame to a Google Sheet. fill_nans Fill NaN values in a DataFrame column or all numeric columns using either 'perlin' noise or 'linear' regression. df : pandas.DataFrame Input DataFrame containing numeric columns. column_name : str, optional Column to fill. If None, all numeric columns will be filled. method : str, default \"perlin\" Filling method: \"perlin\" for Perlin noise, \"linear\" for linear regression. seed : int, default 0 Seed or starting index for Perlin noise generation. scale_factor : float, default 0.1 Step size for Perlin noise generation. lim_min : float, default 0 Minimum limit for filled values. lim_max : float, default 100 Maximum limit for filled values. pandas.DataFrame A copy of the original DataFrame with NaNs filled, respecting user-defined limits. Raises: - TypeError: If the specified column is not numeric. - ValueError: If the method is not \"perlin\" or \"linear\". Source code in ironBars/ironSheets.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def fill_nans ( df , column_name = None , method = \"perlin\" , seed = 0 , scale_factor = 0.1 , lim_min = 0 , lim_max = 100 ): \"\"\" Fill NaN values in a DataFrame column or all numeric columns using either 'perlin' noise or 'linear' regression. Parameters: - df : pandas.DataFrame Input DataFrame containing numeric columns. - column_name : str, optional Column to fill. If None, all numeric columns will be filled. - method : str, default \"perlin\" Filling method: \"perlin\" for Perlin noise, \"linear\" for linear regression. - seed : int, default 0 Seed or starting index for Perlin noise generation. - scale_factor : float, default 0.1 Step size for Perlin noise generation. - lim_min : float, default 0 Minimum limit for filled values. - lim_max : float, default 100 Maximum limit for filled values. Returns: - pandas.DataFrame A copy of the original DataFrame with NaNs filled, respecting user-defined limits. Raises: - TypeError: If the specified column is not numeric. - ValueError: If the method is not \"perlin\" or \"linear\". \"\"\" df_copy = df . copy () if column_name is not None : if not np . issubdtype ( df_copy [ column_name ] . dtype , np . number ): raise TypeError ( f \"Column ' { column_name } ' must be numeric.\" ) cols_to_fill = [ column_name ] else : cols_to_fill = df_copy . select_dtypes ( include = [ np . number ]) . columns . tolist () for col in cols_to_fill : arr = df_copy [ col ] . to_numpy ( dtype = float ) nan_mask = np . isnan ( arr ) if not nan_mask . any (): continue if method == \"perlin\" : mean_val = np . nanmean ( arr ) std_val = np . nanstd ( arr ) indices = np . arange ( len ( arr )) noise_vals = np . array ([ pnoise1 (( i + seed ) * scale_factor ) for i in indices ]) noise_scaled = noise_vals * 2 * std_val + mean_val noise_scaled = np . clip ( noise_scaled , lim_min , lim_max ) arr [ nan_mask ] = noise_scaled [ nan_mask ] elif method == \"linear\" : indices_all = np . arange ( len ( arr )) . reshape ( - 1 , 1 ) X_train = indices_all [ ~ nan_mask ] y_train = arr [ ~ nan_mask ] X_pred = indices_all [ nan_mask ] model = LinearRegression () model . fit ( X_train , y_train ) arr [ nan_mask ] = model . predict ( X_pred ) arr = np . clip ( arr , lim_min , lim_max ) else : raise ValueError ( \"Method must be 'perlin' or 'linear'\" ) df_copy [ col ] = arr return df_copy Fill missing values in a DataFrame using Perlin Noise and Linear Regression.","title":"IronBars Documentation"},{"location":"#ironbars-documentation","text":"","title":"IronBars Documentation"},{"location":"#gsheet_load","text":"Convert a Google Sheets, GitHub, or direct CSV link to a CSV-ready URL and optionally load as DataFrame(s). Parameters: Name Type Description Default url str | list URL(s) of CSV file(s). required as_df bool If True, return pandas DataFrame(s) instead of URL(s). False max_workers int | None Max threads for parallel downloading (only for list of URLs). None Returns: Type Description str | list | pd.DataFrame | list[pd.DataFrame] Source code in ironBars/ironSheets.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def gsheet_load ( url , as_df = False , max_workers = None ): \"\"\" Convert a Google Sheets, GitHub, or direct CSV link to a CSV-ready URL and optionally load as DataFrame(s). Parameters: url (str | list): URL(s) of CSV file(s). as_df (bool): If True, return pandas DataFrame(s) instead of URL(s). max_workers (int | None): Max threads for parallel downloading (only for list of URLs). Returns: str | list | pd.DataFrame | list[pd.DataFrame] \"\"\" def convert_url ( single_url ): parsed = urlparse ( single_url ) netloc = parsed . netloc . lower () path = parsed . path . lower () # Google Sheets if \"docs.google.com\" in netloc and \"/spreadsheets/\" in path : if \"/edit\" in single_url : return single_url . split ( \"/edit\" )[ 0 ] + \"/gviz/tq?tqx=out:csv\" else : return single_url # GitHub elif \"github.com\" in netloc : if \"/blob/\" in single_url : return single_url . replace ( \"github.com\" , \"raw.githubusercontent.com\" ) . replace ( \"/blob/\" , \"/\" ) return single_url # Already raw URL # Direct CSV link else : return single_url def fetch_csv ( single_url ): csv_url = convert_url ( single_url ) try : return pd . read_csv ( csv_url ) except Exception as e : raise RuntimeError ( f \"Failed to read CSV from { single_url } : { e } \" ) # Handle single URL vs list if isinstance ( url , list ): if as_df : with ThreadPoolExecutor ( max_workers = max_workers ) as executor : return list ( executor . map ( fetch_csv , url )) else : return [ convert_url ( u ) for u in url ] else : converted = convert_url ( url ) return pd . read_csv ( converted ) if as_df else converted Load data from a Google Sheet into a pandas DataFrame.","title":"gsheet_load"},{"location":"#gsheet_save","text":"Save one or more DataFrames as CSV files. Parameters: Name Type Description Default data_frames DataFrame | list [ DataFrame ] Single dataframe or list of dataframes to save. required auto_name bool If True, generates filenames automatically using name_series + index (only for multiple frames). True name_series str | list Base name (str) or list of names (required if auto_name=False and multiple frames). 'Sheet' save_dir str Directory to save CSV files. Defaults to current directory. '.' filename str | None Filename for single dataframe. Required if saving a single DataFrame. None Source code in ironBars/ironSheets.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def gsheet_save ( data_frames , auto_name = True , name_series = \"Sheet\" , save_dir = \".\" , filename = None ): \"\"\" Save one or more DataFrames as CSV files. Parameters: data_frames (pd.DataFrame | list[pd.DataFrame]): Single dataframe or list of dataframes to save. auto_name (bool): If True, generates filenames automatically using name_series + index (only for multiple frames). name_series (str | list): Base name (str) or list of names (required if auto_name=False and multiple frames). save_dir (str): Directory to save CSV files. Defaults to current directory. filename (str | None): Filename for single dataframe. Required if saving a single DataFrame. \"\"\" # Normalize input to list single_input = False if isinstance ( data_frames , pd . DataFrame ): data_frames = [ data_frames ] single_input = True assert data_frames , \"DATA FRAMES must not be empty!\" # Ensure absolute save directory path save_dir = os . path . abspath ( save_dir ) os . makedirs ( save_dir , exist_ok = True ) # Handle single dataframe case if single_input : assert filename , \"Must provide 'filename' when saving a single DataFrame\" if not filename . lower () . endswith ( \".csv\" ): filename += \".csv\" fpath = os . path . join ( save_dir , filename ) data_frames [ 0 ] . to_csv ( fpath , index = False ) return # Nothing to return # Handle multiple dataframes if auto_name : if isinstance ( name_series , list ): raise RuntimeError ( \"Auto Name -> True but name_series is a list! Provide a string base name instead.\" ) else : if not isinstance ( name_series , list ): raise RuntimeError ( \"Auto Name -> False. Provide a list of filenames as name_series.\" ) if len ( name_series ) < len ( data_frames ): raise ValueError ( \"Number of filenames provided is less than number of data frames.\" ) for idx , frame in enumerate ( data_frames ): if auto_name : fname = f \" { name_series }{ idx } .csv\" else : fname = f \" { name_series [ idx ] } .csv\" fpath = os . path . join ( save_dir , fname ) frame . to_csv ( fpath , index = False ) Save a pandas DataFrame to a Google Sheet.","title":"gsheet_save"},{"location":"#fill_nans","text":"Fill NaN values in a DataFrame column or all numeric columns using either 'perlin' noise or 'linear' regression. df : pandas.DataFrame Input DataFrame containing numeric columns. column_name : str, optional Column to fill. If None, all numeric columns will be filled. method : str, default \"perlin\" Filling method: \"perlin\" for Perlin noise, \"linear\" for linear regression. seed : int, default 0 Seed or starting index for Perlin noise generation. scale_factor : float, default 0.1 Step size for Perlin noise generation. lim_min : float, default 0 Minimum limit for filled values. lim_max : float, default 100 Maximum limit for filled values. pandas.DataFrame A copy of the original DataFrame with NaNs filled, respecting user-defined limits. Raises: - TypeError: If the specified column is not numeric. - ValueError: If the method is not \"perlin\" or \"linear\". Source code in ironBars/ironSheets.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def fill_nans ( df , column_name = None , method = \"perlin\" , seed = 0 , scale_factor = 0.1 , lim_min = 0 , lim_max = 100 ): \"\"\" Fill NaN values in a DataFrame column or all numeric columns using either 'perlin' noise or 'linear' regression. Parameters: - df : pandas.DataFrame Input DataFrame containing numeric columns. - column_name : str, optional Column to fill. If None, all numeric columns will be filled. - method : str, default \"perlin\" Filling method: \"perlin\" for Perlin noise, \"linear\" for linear regression. - seed : int, default 0 Seed or starting index for Perlin noise generation. - scale_factor : float, default 0.1 Step size for Perlin noise generation. - lim_min : float, default 0 Minimum limit for filled values. - lim_max : float, default 100 Maximum limit for filled values. Returns: - pandas.DataFrame A copy of the original DataFrame with NaNs filled, respecting user-defined limits. Raises: - TypeError: If the specified column is not numeric. - ValueError: If the method is not \"perlin\" or \"linear\". \"\"\" df_copy = df . copy () if column_name is not None : if not np . issubdtype ( df_copy [ column_name ] . dtype , np . number ): raise TypeError ( f \"Column ' { column_name } ' must be numeric.\" ) cols_to_fill = [ column_name ] else : cols_to_fill = df_copy . select_dtypes ( include = [ np . number ]) . columns . tolist () for col in cols_to_fill : arr = df_copy [ col ] . to_numpy ( dtype = float ) nan_mask = np . isnan ( arr ) if not nan_mask . any (): continue if method == \"perlin\" : mean_val = np . nanmean ( arr ) std_val = np . nanstd ( arr ) indices = np . arange ( len ( arr )) noise_vals = np . array ([ pnoise1 (( i + seed ) * scale_factor ) for i in indices ]) noise_scaled = noise_vals * 2 * std_val + mean_val noise_scaled = np . clip ( noise_scaled , lim_min , lim_max ) arr [ nan_mask ] = noise_scaled [ nan_mask ] elif method == \"linear\" : indices_all = np . arange ( len ( arr )) . reshape ( - 1 , 1 ) X_train = indices_all [ ~ nan_mask ] y_train = arr [ ~ nan_mask ] X_pred = indices_all [ nan_mask ] model = LinearRegression () model . fit ( X_train , y_train ) arr [ nan_mask ] = model . predict ( X_pred ) arr = np . clip ( arr , lim_min , lim_max ) else : raise ValueError ( \"Method must be 'perlin' or 'linear'\" ) df_copy [ col ] = arr return df_copy Fill missing values in a DataFrame using Perlin Noise and Linear Regression.","title":"fill_nans"}]}